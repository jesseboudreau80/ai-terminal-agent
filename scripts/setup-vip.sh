
#!/usr/bin/env bash

set -euo pipefail



echo ""

echo "üêæ Welcome to the AI Terminal Agent ‚Äî VIP Setup!"

echo "-----------------------------------------------"

echo "This script will automatically build llama.cpp, download models,"

echo "and launch your first local AI chat experience."

echo ""



cd "$(dirname "$0")/.."



# 1Ô∏è‚É£ Check dependencies

echo "üì¶ Checking dependencies..."

sudo apt update -y

sudo apt install -y git cmake build-essential curl wget



# 2Ô∏è‚É£ Initialize llama.cpp submodule

echo ""

echo "üîß Setting up llama.cpp..."

git submodule update --init --recursive



cd llama.cpp

if [ ! -d build ]; then

  mkdir build

fi

cd build

cmake ..

make -j$(nproc)

cd ../..



# 3Ô∏è‚É£ Create models directory

mkdir -p models



# 4Ô∏è‚É£ Download TinyLlama (lightweight model for fast startup)

if [ ! -f "models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf" ]; then

  echo ""

  echo "‚¨áÔ∏è Downloading TinyLlama model (~600MB)..."

  wget -O models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf \

    https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf

else

  echo "‚úÖ TinyLlama already present."

fi



# 5Ô∏è‚É£ Run a test chat

echo ""

echo "üöÄ Launching your first AI chat..."

bash scripts/chat-switcher.sh tinyllama



echo ""

echo "‚ú® Setup complete! You can now run any of these:"

echo "  bash scripts/chat-switcher.sh mistral"

echo "  bash scripts/chat-switcher.sh codellama"

echo "  bash scripts/chat-switcher.sh phi2"

echo "  bash scripts/chat-switcher.sh mythomax"

echo ""

echo "üê∂ Run it local. Run it fast. Run it private. ‚Äî üß† AI Terminal Agent"

